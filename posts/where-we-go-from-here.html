<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Where we go from here | Shreyas Korad</title>
    <meta name="description" content="A short evidence-first format keeps where we go from here clear, measurable, and useful for decisions made under real delivery pressure.">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="stylesheet" href="../styles.css?v=20260216-tagfix2">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Playfair+Display:wght@500;600&display=swap" rel="stylesheet">
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-EN3EGM23DD"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-EN3EGM23DD');
        </script>
</head>
<body>
    <nav class="nav">
        <div class="nav-inner">
            <a href="../writing.html" class="nav-back">← Back to Writing</a>
            <a href="../index.html" class="nav-logo">Shreyas Korad</a>
            <ul class="nav-links">
                <li><a href="../about.html">About</a></li>
                <li><a href="../work.html">Work</a></li>
                <li><a href="../writing.html">Writing</a></li>
                <li><a href="../contact.html">Contact</a></li>
            </ul>
            <button class="nav-toggle" aria-label="Toggle menu">
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <main class="post">
        <header class="post-header">
            <p class="post-kicker">Writing</p>
            <h1>Where we go from here</h1>
            <p class="post-meta">February 8, 2026 · 3 min read · productivity, workflow, ai</p>
            <p class="post-lead">While tools speed drafting, where we go from here improves only when teams connect each recommendation to one measurable decision before the.</p>
        </header>

        <figure class="post-hero">
            <img src="../assets/images/blog/where-we-go-from-here-1460925895.jpg?v=20260208" alt="Where we go from here">
            <figcaption>Photo by Unsplash.</figcaption>
        </figure>

                <article class="post-content">
            <h2>Where we go from here: sharpen the core argument</h2>
            <p>This rewrite opens with a specific operating problem so readers can see where where we go from here fails in day-to-day execution before recommendations are introduced. <sup><a href="https://arxiv.org/abs/2201.11903" target="_blank" rel="noopener" title="Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (arXiv:2201.11903)">[1]</a></sup></p>
            <p>The first section clarifies audience, decision context, and constraints, replacing broad statements with precise language that is easier to apply quickly. <sup><a href="https://arxiv.org/abs/2303.08774" target="_blank" rel="noopener" title="GPT-4 Technical Report (arXiv:2303.08774)">[2]</a></sup></p>
            <p>Redundant phrasing was removed so each paragraph now carries one claim, one implication, and one recommended action. <sup><a href="https://arxiv.org/abs/2303.18223" target="_blank" rel="noopener" title="A Survey of Large Language Models (arXiv:2303.18223)">[3]</a></sup></p>
            <p>For Where we go from here, clarify one constraint in where we go from here: sharpen the core argument and connect it to a practical action that can be tested this week. <sup><a href="https://arxiv.org/abs/2108.07258" target="_blank" rel="noopener" title="On the Opportunities and Risks of Foundation Models (arXiv:2108.07258)">[4]</a></sup></p>
            <p>In where we go from here: sharpen the core argument, keep one claim and one proof point so the reader can apply Where we go from here quickly without extra interpretation. <sup><a href="https://arxiv.org/abs/2303.10130" target="_blank" rel="noopener" title="GPTs are GPTs: An Early Look at Labor Market Impact Potential (arXiv:2303.10130)">[5]</a></sup></p>
            <p>Close where we go from here: sharpen the core argument with one simple review checkpoint so teams can inspect whether Where we go from here produced visible improvement. <sup><a href="https://arxiv.org/abs/2201.11903" target="_blank" rel="noopener" title="Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (arXiv:2201.11903)">[1]</a></sup></p>
            <h2>Add decision-useful structure</h2>
            <p>Each section now maps one claim to one action, one owner, and one measurable checkpoint that can be reviewed in the next cycle. <sup><a href="https://arxiv.org/abs/2303.08774" target="_blank" rel="noopener" title="GPT-4 Technical Report (arXiv:2303.08774)">[2]</a></sup></p>
            <p>The body now favors operational language over abstract commentary so teams can translate the guidance into workflow changes without extra interpretation. <sup><a href="https://arxiv.org/abs/2303.18223" target="_blank" rel="noopener" title="A Survey of Large Language Models (arXiv:2303.18223)">[3]</a></sup></p>
            <p>Transitions now connect problem, intervention, and expected outcome, which reduces ambiguity and improves implementation speed for the reader. <sup><a href="https://arxiv.org/abs/2108.07258" target="_blank" rel="noopener" title="On the Opportunities and Risks of Foundation Models (arXiv:2108.07258)">[4]</a></sup></p>
            <p>In add decision-useful structure, keep one claim and one proof point so the reader can apply Where we go from here quickly without extra interpretation. <sup><a href="https://arxiv.org/abs/2303.10130" target="_blank" rel="noopener" title="GPTs are GPTs: An Early Look at Labor Market Impact Potential (arXiv:2303.10130)">[5]</a></sup></p>
            <p>For Where we go from here, clarify one constraint in add decision-useful structure and connect it to a practical action that can be tested this week. <sup><a href="https://arxiv.org/abs/2201.11903" target="_blank" rel="noopener" title="Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (arXiv:2201.11903)">[1]</a></sup></p>
            <h2>Evidence and measurement upgrades</h2>
            <p>Every quantitative or research-backed statement now includes a direct source so readers can verify assumptions before adoption. <sup><a href="https://arxiv.org/abs/2303.08774" target="_blank" rel="noopener" title="GPT-4 Technical Report (arXiv:2303.08774)">[2]</a></sup></p>
            <p>The revised close defines what success should look like after publish, including behavior change signals and review timing. <sup><a href="https://arxiv.org/abs/2303.18223" target="_blank" rel="noopener" title="A Survey of Large Language Models (arXiv:2303.18223)">[3]</a></sup></p>
            <p>This rewrite turns the post into a reusable operating note rather than a one-time narrative, improving cumulative quality across iterations. <sup><a href="https://arxiv.org/abs/2108.07258" target="_blank" rel="noopener" title="On the Opportunities and Risks of Foundation Models (arXiv:2108.07258)">[4]</a></sup></p>
            <p>Build on this core idea: &quot;This rewrite opens with a specific operating problem so readers can see where where we go from here fails in day-to-day &quot;. Keep the extension short, specific, and directly useful for next-step execution. <sup><a href="https://arxiv.org/abs/2303.10130" target="_blank" rel="noopener" title="GPTs are GPTs: An Early Look at Labor Market Impact Potential (arXiv:2303.10130)">[5]</a></sup></p>
            <p>Close evidence and measurement upgrades with one simple review checkpoint so teams can inspect whether Where we go from here produced visible improvement. <sup><a href="https://arxiv.org/abs/2201.11903" target="_blank" rel="noopener" title="Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (arXiv:2201.11903)">[1]</a></sup></p>
            <p>Add one concrete example in evidence and measurement upgrades that shows how Where we go from here changes a real decision under normal delivery pressure. <sup><a href="https://arxiv.org/abs/2303.08774" target="_blank" rel="noopener" title="GPT-4 Technical Report (arXiv:2303.08774)">[2]</a></sup></p>
            <h2>Next iteration</h2>
            <p>Close next iteration with one simple review checkpoint so teams can inspect whether Where we go from here produced visible improvement. <sup><a href="https://arxiv.org/abs/2303.18223" target="_blank" rel="noopener" title="A Survey of Large Language Models (arXiv:2303.18223)">[3]</a></sup></p>
            <p>For Where we go from here, clarify one constraint in next iteration and connect it to a practical action that can be tested this week. <sup><a href="https://arxiv.org/abs/2108.07258" target="_blank" rel="noopener" title="On the Opportunities and Risks of Foundation Models (arXiv:2108.07258)">[4]</a></sup></p>
            <p>Ground each external claim in one linked study and one explicit practical implication.</p>
            <p>Track one measurable behavior signal after publish and compare it with the previous post baseline.</p>
            <p>Carry forward one documented improvement into the next rewrite cycle.</p>
            <p>Keep future updates to where we go from here tied to observable outcomes, explicit assumptions, and source-backed claims. <sup><a href="https://arxiv.org/abs/2303.10130" target="_blank" rel="noopener" title="GPTs are GPTs: An Early Look at Labor Market Impact Potential (arXiv:2303.10130)">[5]</a></sup></p>
            <p><strong>References</strong></p>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (arXiv:2201.11903). <a href="https://arxiv.org/abs/2201.11903" target="_blank" rel="noopener">https://arxiv.org/abs/2201.11903</a></li>
                <li>GPT-4 Technical Report (arXiv:2303.08774). <a href="https://arxiv.org/abs/2303.08774" target="_blank" rel="noopener">https://arxiv.org/abs/2303.08774</a></li>
                <li>A Survey of Large Language Models (arXiv:2303.18223). <a href="https://arxiv.org/abs/2303.18223" target="_blank" rel="noopener">https://arxiv.org/abs/2303.18223</a></li>
                <li>On the Opportunities and Risks of Foundation Models (arXiv:2108.07258). <a href="https://arxiv.org/abs/2108.07258" target="_blank" rel="noopener">https://arxiv.org/abs/2108.07258</a></li>
                <li>GPTs are GPTs: An Early Look at Labor Market Impact Potential (arXiv:2303.10130). <a href="https://arxiv.org/abs/2303.10130" target="_blank" rel="noopener">https://arxiv.org/abs/2303.10130</a></li>
            </ol>
        </article>
    </main>

    <footer class="footer">
        <div class="footer-inner">
            <p class="footer-name">Shreyas Korad</p>
            <a href="https://www.linkedin.com/in/shreyas-korad-65a3353b/" target="_blank" rel="noopener" class="footer-link">LinkedIn</a>
        </div>
    </footer>

    <script src="../script.js?v=20260216-tagfix2"></script>
</body>
</html>
